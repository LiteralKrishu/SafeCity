{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 271, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/genkit.ts"],"sourcesContent":["import {genkit} from 'genkit';\nimport {googleAI} from '@genkit-ai/google-genai';\n\nexport const ai = genkit({\n  plugins: [googleAI()],\n  model: 'googleai/gemini-2.5-flash',\n});\n"],"names":[],"mappings":";;;AAAA;AAAA;AACA;AAAA;;;AAEO,MAAM,KAAK,CAAA,GAAA,uIAAA,CAAA,SAAM,AAAD,EAAE;IACvB,SAAS;QAAC,CAAA,GAAA,6KAAA,CAAA,WAAQ,AAAD;KAAI;IACrB,OAAO;AACT","debugId":null}},
    {"offset": {"line": 292, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/analyze-video-frame.ts"],"sourcesContent":["'use server';\n\n/**\n * @fileOverview This file defines a Genkit flow for analyzing a video frame for signs of distress.\n *\n * The flow takes an image data URI, and analyzes it for emotional distress or haphazard movement.\n * @interface AnalyzeVideoFrameInput - Input interface for the analyzeVideoFrame function.\n * @interface AnalyzeVideoFrameOutput - Output interface for the analyzeVideoFrame function.\n * @function analyzeVideoFrame - The main function to analyze a video frame.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst AnalyzeVideoFrameInputSchema = z.object({\n  frameDataUri: z\n    .string()\n    .describe(\n      \"A video frame image as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'.\"\n    ),\n});\nexport type AnalyzeVideoFrameInput = z.infer<typeof AnalyzeVideoFrameInputSchema>;\n\nconst AnalyzeVideoFrameOutputSchema = z.object({\n  isHaphazardMovement: z\n    .boolean()\n    .describe('Whether or not the frame contains sudden, erratic, or haphazard movements that could indicate a struggle or fall.'),\n  detectedEmotion: z\n    .enum(['anger', 'fear', 'sadness', 'joy', 'surprise', 'disgust', 'neutral'])\n    .describe('The dominant emotion detected from facial expressions in the frame. If no face is detected, return neutral.'),\n});\nexport type AnalyzeVideoFrameOutput = z.infer<typeof AnalyzeVideoFrameOutputSchema>;\n\nexport async function analyzeVideoFrame(input: AnalyzeVideoFrameInput): Promise<AnalyzeVideoFrameOutput> {\n  return analyzeVideoFrameFlow(input);\n}\n\nconst analyzeVideoFramePrompt = ai.definePrompt({\n  name: 'analyzeVideoFramePrompt',\n  input: {schema: AnalyzeVideoFrameInputSchema},\n  output: {schema: AnalyzeVideoFrameOutputSchema},\n  prompt: `You are a security expert analyzing a video frame for signs of distress.\n  \nAnalyze the following image for:\n1.  **Haphazard Movement**: Determine if the subject's posture or the scene suggests a sudden, uncontrolled movement, like a fall, a struggle, or erratic action. Set 'isHaphazardMovement' to true if detected.\n2.  **Emotion**: Analyze the facial expression of any person in the frame. Identify the dominant emotion. If multiple people are present, focus on the one showing the most distress. If no face is visible, default to 'neutral'.\n\nImage: {{media url=frameDataUri}}`,\n});\n\nconst analyzeVideoFrameFlow = ai.defineFlow(\n  {\n    name: 'analyzeVideoFrameFlow',\n    inputSchema: AnalyzeVideoFrameInputSchema,\n    outputSchema: AnalyzeVideoFrameOutputSchema,\n  },\n  async input => {\n    try {\n      const {output} = await analyzeVideoFramePrompt(input);\n      return output!;\n    } catch(e) {\n      console.error(\"Error analyzing video frame: \", e);\n      // In case of an error, assume no threat.\n      return { isHaphazardMovement: false, detectedEmotion: 'neutral' };\n    }\n  }\n);\n"],"names":[],"mappings":";;;;;AAEA;;;;;;;CAOC,GAED;AACA;AAAA;;;;;;AAEA,MAAM,+BAA+B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC5C,cAAc,uIAAA,CAAA,IAAC,CACZ,MAAM,GACN,QAAQ,CACP;AAEN;AAGA,MAAM,gCAAgC,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC7C,qBAAqB,uIAAA,CAAA,IAAC,CACnB,OAAO,GACP,QAAQ,CAAC;IACZ,iBAAiB,uIAAA,CAAA,IAAC,CACf,IAAI,CAAC;QAAC;QAAS;QAAQ;QAAW;QAAO;QAAY;QAAW;KAAU,EAC1E,QAAQ,CAAC;AACd;AAGO,eAAe,kBAAkB,KAA6B;IACnE,OAAO,sBAAsB;AAC/B;AAEA,MAAM,0BAA0B,mHAAA,CAAA,KAAE,CAAC,YAAY,CAAC;IAC9C,MAAM;IACN,OAAO;QAAC,QAAQ;IAA4B;IAC5C,QAAQ;QAAC,QAAQ;IAA6B;IAC9C,QAAQ,CAAC;;;;;;iCAMsB,CAAC;AAClC;AAEA,MAAM,wBAAwB,mHAAA,CAAA,KAAE,CAAC,UAAU,CACzC;IACE,MAAM;IACN,aAAa;IACb,cAAc;AAChB,GACA,OAAM;IACJ,IAAI;QACF,MAAM,EAAC,MAAM,EAAC,GAAG,MAAM,wBAAwB;QAC/C,OAAO;IACT,EAAE,OAAM,GAAG;QACT,QAAQ,KAAK,CAAC,iCAAiC;QAC/C,yCAAyC;QACzC,OAAO;YAAE,qBAAqB;YAAO,iBAAiB;QAAU;IAClE;AACF;;;IAhCoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 374, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/analyze-audio-for-distress.ts"],"sourcesContent":["'use server';\n\n/**\n * @fileOverview This file defines a Genkit flow for analyzing audio for signs of distress.\n *\n * The flow takes audio data, transcribes it, and analyzes the text for distress signals.\n * @interface AnalyzeAudioForDistressInput - Input interface for the analyzeAudioForDistress function.\n * @interface AnalyzeAudioForDistressOutput - Output interface for the analyzeAudioForDistress function.\n * @function analyzeAudioForDistress - The main function to analyze audio.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst AnalyzeAudioForDistressInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      \"A chunk of audio as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'.\"\n    ),\n});\nexport type AnalyzeAudioForDistressInput = z.infer<typeof AnalyzeAudioForDistressInputSchema>;\n\nconst AnalyzeAudioForDistressOutputSchema = z.object({\n  isDistress: z\n    .boolean()\n    .describe('Whether or not the audio contains signs of distress, such as screaming, fear, or urgent pleas for help.'),\n});\nexport type AnalyzeAudioForDistressOutput = z.infer<typeof AnalyzeAudioForDistressOutputSchema>;\n\nexport async function analyzeAudioForDistress(input: AnalyzeAudioForDistressInput): Promise<AnalyzeAudioForDistressOutput> {\n  return analyzeAudioForDistressFlow(input);\n}\n\nconst analyzeAudioPrompt = ai.definePrompt({\n  name: 'analyzeAudioPrompt',\n  input: {schema: AnalyzeAudioForDistressInputSchema},\n  output: {schema: AnalyzeAudioForDistressOutputSchema},\n  prompt: `You are a security expert trained to detect signs of distress in audio.\n  \nTranscribe the following audio and analyze the transcription. Determine if the content contains signs of distress.\nThis could include screaming, phrases of fear, urgent pleas for help, or other indicators that the person is in danger.\n\nSet isDistress to true if distress is detected.\n\nAudio: {{media url=audioDataUri}}`,\n});\n\nconst analyzeAudioForDistressFlow = ai.defineFlow(\n  {\n    name: 'analyzeAudioForDistressFlow',\n    inputSchema: AnalyzeAudioForDistressInputSchema,\n    outputSchema: AnalyzeAudioForDistressOutputSchema,\n  },\n  async input => {\n    try {\n      const {output} = await analyzeAudioPrompt(input);\n      return output!;\n    } catch(e) {\n      console.error(\"Error analyzing audio: \", e);\n      // In case of an error from the LLM, assume no distress.\n      return { isDistress: false };\n    }\n  }\n);\n"],"names":[],"mappings":";;;;;AAEA;;;;;;;CAOC,GAED;AACA;AAAA;;;;;;AAEA,MAAM,qCAAqC,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAClD,cAAc,uIAAA,CAAA,IAAC,CACZ,MAAM,GACN,QAAQ,CACP;AAEN;AAGA,MAAM,sCAAsC,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACnD,YAAY,uIAAA,CAAA,IAAC,CACV,OAAO,GACP,QAAQ,CAAC;AACd;AAGO,eAAe,wBAAwB,KAAmC;IAC/E,OAAO,4BAA4B;AACrC;AAEA,MAAM,qBAAqB,mHAAA,CAAA,KAAE,CAAC,YAAY,CAAC;IACzC,MAAM;IACN,OAAO;QAAC,QAAQ;IAAkC;IAClD,QAAQ;QAAC,QAAQ;IAAmC;IACpD,QAAQ,CAAC;;;;;;;iCAOsB,CAAC;AAClC;AAEA,MAAM,8BAA8B,mHAAA,CAAA,KAAE,CAAC,UAAU,CAC/C;IACE,MAAM;IACN,aAAa;IACb,cAAc;AAChB,GACA,OAAM;IACJ,IAAI;QACF,MAAM,EAAC,MAAM,EAAC,GAAG,MAAM,mBAAmB;QAC1C,OAAO;IACT,EAAE,OAAM,GAAG;QACT,QAAQ,KAAK,CAAC,2BAA2B;QACzC,wDAAwD;QACxD,OAAO;YAAE,YAAY;QAAM;IAC7B;AACF;;;IAjCoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 447, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/.next-internal/server/app/page/actions.js%20%28server%20actions%20loader%29"],"sourcesContent":["export {analyzeVideoFrame as '40c67a26c40ffb44c5015e3e7fe844c851d48b90f2'} from 'ACTIONS_MODULE0'\nexport {analyzeAudioForDistress as '405d0ae66e0ea7d30de10db6372f3404aaf00ea520'} from 'ACTIONS_MODULE1'\n"],"names":[],"mappings":";AAAA;AACA","debugId":null}},
    {"offset": {"line": 505, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/page.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport default registerClientReference(\n    function() { throw new Error(\"Attempted to call the default export of [project]/src/app/page.tsx <module evaluation> from the server, but it's on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/app/page.tsx <module evaluation>\",\n    \"default\",\n);\n"],"names":[],"mappings":";;;AAAA;;uCACe,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjC;IAAa,MAAM,IAAI,MAAM;AAAoR,GACjT,kDACA","debugId":null}},
    {"offset": {"line": 519, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/page.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport default registerClientReference(\n    function() { throw new Error(\"Attempted to call the default export of [project]/src/app/page.tsx from the server, but it's on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/app/page.tsx\",\n    \"default\",\n);\n"],"names":[],"mappings":";;;AAAA;;uCACe,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjC;IAAa,MAAM,IAAI,MAAM;AAAgQ,GAC7R,8BACA","debugId":null}},
    {"offset": {"line": 533, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}}]
}