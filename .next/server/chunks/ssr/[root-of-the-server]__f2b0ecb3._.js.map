{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 271, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/genkit.ts"],"sourcesContent":["import {genkit} from 'genkit';\nimport {googleAI} from '@genkit-ai/google-genai';\n\nexport const ai = genkit({\n  plugins: [googleAI()],\n  model: 'googleai/gemini-2.5-flash',\n});\n"],"names":[],"mappings":";;;AAAA;AAAA;AACA;AAAA;;;AAEO,MAAM,KAAK,CAAA,GAAA,uIAAA,CAAA,SAAM,AAAD,EAAE;IACvB,SAAS;QAAC,CAAA,GAAA,6KAAA,CAAA,WAAQ,AAAD;KAAI;IACrB,OAAO;AACT","debugId":null}},
    {"offset": {"line": 292, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/analyze-audio-for-distress.ts"],"sourcesContent":["'use server';\n\n/**\n * @fileOverview This file defines a Genkit flow for analyzing audio for signs of distress.\n *\n * The flow takes audio data, transcribes it, and analyzes the text for distress signals.\n * @interface AnalyzeAudioForDistressInput - Input interface for the analyzeAudioForDistress function.\n * @interface AnalyzeAudioForDistressOutput - Output interface for the analyzeAudioForDistress function.\n * @function analyzeAudioForDistress - The main function to analyze audio.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst AnalyzeAudioForDistressInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      \"A chunk of audio as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'.\"\n    ),\n});\nexport type AnalyzeAudioForDistressInput = z.infer<typeof AnalyzeAudioForDistressInputSchema>;\n\nconst AnalyzeAudioForDistressOutputSchema = z.object({\n  isDistress: z\n    .boolean()\n    .describe('Whether or not the audio contains signs of distress, such as screaming, fear, or urgent pleas for help.'),\n});\nexport type AnalyzeAudioForDistressOutput = z.infer<typeof AnalyzeAudioForDistressOutputSchema>;\n\nexport async function analyzeAudioForDistress(input: AnalyzeAudioForDistressInput): Promise<AnalyzeAudioForDistressOutput> {\n  return analyzeAudioForDistressFlow(input);\n}\n\nconst analyzeAudioPrompt = ai.definePrompt({\n  name: 'analyzeAudioPrompt',\n  input: {schema: AnalyzeAudioForDistressInputSchema},\n  output: {schema: AnalyzeAudioForDistressOutputSchema},\n  prompt: `You are a security expert trained to detect signs of distress in audio.\n  \nTranscribe the following audio and analyze the transcription. Determine if the content contains signs of distress.\nThis could include screaming, phrases of fear, urgent pleas for help, or other indicators that the person is in danger.\n\nSet isDistress to true if distress is detected.\n\nAudio: {{media url=audioDataUri}}`,\n});\n\nconst analyzeAudioForDistressFlow = ai.defineFlow(\n  {\n    name: 'analyzeAudioForDistressFlow',\n    inputSchema: AnalyzeAudioForDistressInputSchema,\n    outputSchema: AnalyzeAudioForDistressOutputSchema,\n  },\n  async input => {\n    try {\n      const {output} = await analyzeAudioPrompt(input);\n      return output!;\n    } catch(e) {\n      console.error(\"Error analyzing audio: \", e);\n      // In case of an error from the LLM, assume no distress.\n      return { isDistress: false };\n    }\n  }\n);\n"],"names":[],"mappings":";;;;;AAEA;;;;;;;CAOC,GAED;AACA;AAAA;;;;;;AAEA,MAAM,qCAAqC,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAClD,cAAc,uIAAA,CAAA,IAAC,CACZ,MAAM,GACN,QAAQ,CACP;AAEN;AAGA,MAAM,sCAAsC,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACnD,YAAY,uIAAA,CAAA,IAAC,CACV,OAAO,GACP,QAAQ,CAAC;AACd;AAGO,eAAe,wBAAwB,KAAmC;IAC/E,OAAO,4BAA4B;AACrC;AAEA,MAAM,qBAAqB,mHAAA,CAAA,KAAE,CAAC,YAAY,CAAC;IACzC,MAAM;IACN,OAAO;QAAC,QAAQ;IAAkC;IAClD,QAAQ;QAAC,QAAQ;IAAmC;IACpD,QAAQ,CAAC;;;;;;;iCAOsB,CAAC;AAClC;AAEA,MAAM,8BAA8B,mHAAA,CAAA,KAAE,CAAC,UAAU,CAC/C;IACE,MAAM;IACN,aAAa;IACb,cAAc;AAChB,GACA,OAAM;IACJ,IAAI;QACF,MAAM,EAAC,MAAM,EAAC,GAAG,MAAM,mBAAmB;QAC1C,OAAO;IACT,EAAE,OAAM,GAAG;QACT,QAAQ,KAAK,CAAC,2BAA2B;QACzC,wDAAwD;QACxD,OAAO;YAAE,YAAY;QAAM;IAC7B;AACF;;;IAjCoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 365, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/.next-internal/server/app/page/actions.js%20%28server%20actions%20loader%29"],"sourcesContent":["export {analyzeAudioForDistress as '405d0ae66e0ea7d30de10db6372f3404aaf00ea520'} from 'ACTIONS_MODULE0'\n"],"names":[],"mappings":";AAAA","debugId":null}},
    {"offset": {"line": 417, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/page.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport default registerClientReference(\n    function() { throw new Error(\"Attempted to call the default export of [project]/src/app/page.tsx <module evaluation> from the server, but it's on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/app/page.tsx <module evaluation>\",\n    \"default\",\n);\n"],"names":[],"mappings":";;;AAAA;;uCACe,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjC;IAAa,MAAM,IAAI,MAAM;AAAoR,GACjT,kDACA","debugId":null}},
    {"offset": {"line": 431, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/page.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport default registerClientReference(\n    function() { throw new Error(\"Attempted to call the default export of [project]/src/app/page.tsx from the server, but it's on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/app/page.tsx\",\n    \"default\",\n);\n"],"names":[],"mappings":";;;AAAA;;uCACe,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjC;IAAa,MAAM,IAAI,MAAM;AAAgQ,GAC7R,8BACA","debugId":null}},
    {"offset": {"line": 445, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}}]
}